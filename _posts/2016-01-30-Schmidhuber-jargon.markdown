---
layout: post
title:  "Schmidhuber Jargon"
date:   2016-01-29 21:10:00
categories: schmidhuber research ai
---



[Jürgen Schmidhuber](http://people.idsia.ch/~juergen/) has written a fair amount of cool stuff.

He has a tendency to use many acronyms in his papers. This post is an attempt to summarize some of them.

Papers: 

 - [On Learning to Think: Algorithmic Information Theory for Novel Combinations of 
Reinforcement Learning Controllers and Recurrent Neural World Models](http://arxiv.org/pdf/1511.09249v1.pdf)
 - [Deep Learning in Neural Networks: An Overview](http://people.idsia.ch/~juergen/DeepLearning2July2014.pdf)
 - [Evolving Large-Scale Neural Networks for Vision-Based Reinforcement Learning](http://people.idsia.ch/~juergen/gecco2013torcs.pdf)

# General

 - `AI` Artificial Intelligence
 - `AGI` Artificial General Intelligence
 - `DL` Deep Learning
 - `RL` Reinforcement Learning
 - `SL` Supervised Learning
 - `UL` Unsupervised Learning

# Networks
 - `NN` Neural Network
 - `ANN` Artificial Neural Network
 - `BNN` Biological Neural Network
 - `CNN` Convolutional Neural Network
 - `FNN` Feedforward Neural Network
 - `RNN` Recurrent Neural Network
 - `BRNN` Bi-directional Recurrent Neural Network
 - `LSTM` Long Short Term Memory
 - `BP` Backpropagation
 - `BPTT` BackPropagation Through Time 
 
# Other
  
 - `AE` AutoEncoder
 - `AIT` Algorithmic information theory
 - `BM` Boltzmann Machine
 - `CAP` Credit Assignment Path
 - `CEC` Constant Error Carousel
 - `CFL` Context Free Language
 - `CM` Controller-Model system
 - `CMA-ES` Covariance Matrix Estimation Evolution Strategies
 - `CoSyNE` Co-Synaptic Neuro-Evolution
 - `CSL` Context Senistive Language
 - `CTC` Connectionist Temporal Classification (gradient-based method to train RNNs) [paper](ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf)
 - `DCT` Discrete Cosine Transform
 - `DBN` Deep Belief Network
 - `DP` Dynamic Programming
 - `DS` Direct Policy Search
 - `EA` Evolutionary Algorithm
 - `EM` Expectation Maximization
 - `FMS` Flat Minimum Search
 - `FSA` Finite State Automaton
 - `GMDH` Group Method of Data Handling
 - `GOFAI` Good Old-Fashioned Artificial Intelligence
 - `GP` Genetic Programming
 - `GPU` Graphics Processing Unit
 - `GP MPCNN`: GPU-Based Max-Pooling Convolutional Neural Network
 - `HMM` Hidden Markov Model
 - `HRL` Hierarchical Reinforcement Learning
 - `HTM` Hierarchical Temporal Memory
 - `HMAX` Hierarchical Model “and X”
 - `HRL` NN-based Hierarchical RL
 - `MC` Multi-Column
 - `MCTS` Monte Carlo Tree Sampling (also Monte Carlo Tree Search)
 - `MDL` Minimum Description Length
 - `MDP` Markov Decision Process
 - `MNIST` Mixed National Institute of Standards and Technology Database
 - `MP` Max-Pooling
 - `MPCNN` Max-Pooling Convolutional Neural Network
 - `NEAT` NeuroEvolution of Augmenting Topologies
 - `NE` NeuroEvolution, (using evolutionary computation to train artifi-cial neural networks) 
   [paper](http://people.idsia.ch/~juergen/gecco2013torcs.pdf)
 - `NES` Natural Evolution Strategies
 - `NFQ` Neural Fitted Q-Learning [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.1193&rep=rep1&type=pdf)
 - `PCC` Potential Causal Connection
 - `PDCC` Potential Direct Causal Connection
 - `PG` Policy Gradient
 - `PM` Predictability Minimization
 - `POMDP` Partially Observable Markov Decision Process
 - `RAAM` Recursive Auto-Associative Memory
 - `RBM` Restricted Boltzmann Machine
 - `ReLU` Rectified Linear Unit
 - `R-prop`: Resilient Backpropagation
 - `RNNAI` RNN-based AI
 - `SLIM NN`: Self-Delimiting Neural Network
 - `SOTA` Self-Organising Tree Algorithm
 - `SVM` Support Vector Machine
 - `TDNN` Time-Delay Neural Network
 - `TIMIT` TI/SRI/MIT Acoustic-Phonetic Continuous Speech Corpus
 - `WTA` Winner-Take-All
 - `TORCS` The Open Racing Car Simulator [wiki](https://en.wikipedia.org/wiki/TORCS)
 
<!--# Institutes-->
 <!--- `IDSIA` Istituto Dalle Molle di Studi sull’Intelligenza Artificiale-->
 <!--- `USI` Universita della Svizzera italiana-->
 <!--- `SUPSI` Scuola universitaria professionale della Svizzera italiana-->
 